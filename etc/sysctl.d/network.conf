# See: https://documentation.suse.com/sbp/all/html/SBP-performance-tuning/index.html#sec-net-stack-tuning
# See: https://wiki.archlinux.org/index.php/sysctl#Networking

# tcp_fastopen is the setting that enables or disables the RFC7413 which allows
# sending and receiving data in the opening SYN packet. Enabling this option
# has the positive effect of not losing the initial handshake packets for
# payload transmission. Thus it maximizes network bandwidth usage.
#
# If this returns a value of 0, it is disabled. Additionally, if it returns a
# value of 1, this means TFO is only enabled on outgoing connections (client),
# and a value of 2 indicates it is only available on listening sockets
# (server). However, you'll want your tcp_fastopen set to 3, which enables
# both.
#
# See: https://www.keycdn.com/support/tcp-fast-open
net.ipv4.tcp_fastopen = 3

# tcp_sack when enabled allows selecting acknowledgments. By default it is
# disabled (value set to 0). It is recommended to enable this option to enhance
# performance.
net.ipv4.tcp_sack = 1

# tcp_lowlatency when enabled (value set to 1) instructs the Linux kernel to
# make decisions that prefer low-latency to high-throughput. By default this
# setting is disabled (value set to 0). It is recommended to enable this option
# in profiles preferring lower latency to higher throughput.
net.ipv4.tcp_lowlatency = 0

# The received frames will be stored in this queue after taking them from the
# ring buffer on the network card.  Increasing this value for high speed cards
# may help prevent losing packets.
net.core.netdev_max_backlog = 16384

# Increase the maximum connections - The upper limit on how many connections
# the kernel will accept (default 128):
net.core.somaxconn = 8192

# Increase the memory dedicated to the network interfaces - By default the
# Linux network stack is not configured for high speed large file transfer
# across WAN links (i.e. handle more network packets) and setting the correct
# values may save memory resources. Increase Linux autotuning TCP buffer limits
# Set max to 16MB for 1GE and 32M (33554432) or 54M (56623104) for 10GE Don't
# set tcp_mem itself! Let the kernel scale it based on RAM.
net.core.rmem_default = 33554432
net.core.rmem_max = 33554432
net.core.wmem_default = 33554432
net.core.wmem_max = 33554432
net.core.optmem_max = 65536
net.ipv4.tcp_rmem = 4096 87380 33554432
net.ipv4.tcp_wmem = 4096 65536 33554432
net.ipv4.udp_rmem_min = 8192
net.ipv4.udp_wmem_min = 8192

# tcp_tw_reuse sets whether TCP should reuse an existing connection in the
# TIME-WAIT state for a new outgoing connection if the new timestamp is
# strictly bigger than the most recent timestamp recorded for the previous
# connection. This helps avoid from running out of available network sockets.
net.ipv4.tcp_tw_reuse = 1

# tcp_slow_start_after_idle sets whether TCP should start at the default window
# size only for new connections or also for existing connections that have been
# idle for too long. This setting kills persistent single connection
# performance and could be turned off.
net.ipv4.tcp_slow_start_after_idle = 0

# TCP keepalive is a mechanism for TCP connections that help to determine
# whether the other end has stopped responding or not. TCP will send the
# keepalive probe that contains null data to the network peer several times
# after a period of idle time. If the peer does not respond, the socket will be
# closed automatically. By default, TCP keepalive process waits for two hours
# (7200 secs) for socket activity before sending the first keepalive probe, and
# then resend it every 75 seconds. As long as there is TCP/IP socket
# communications going on and active, no keepalive packets are needed.
net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_keepalive_intvl = 10
net.ipv4.tcp_keepalive_probes = 6

# The longer the maximum transmission unit (MTU) the better for performance,
# but the worse for reliability. This is because a lost packet means more data
# to be retransmitted and because many routers on the Internet cannot deliver
# very long packets.
net.ipv4.tcp_mtu_probing = 1

# The BBR congestion control algorithm can help achieve higher bandwidths and
# lower latencies for internet traffic. First, load the tcp_bbr module.
net.core.default_qdisc = cake
net.ipv4.tcp_congestion_control = bbr

# Increase ephermeral IP ports
net.ipv4.ip_local_port_range = 10000 65000

# By default, TCP saves various connection metrics in the route cache when the
# connection closes, so that connections established in the near future can use
# these to set initial conditions. Usually, this increases overall performance,
# but may sometimes cause performance degradation. If set, TCP will not cache
# metrics on closing connections.
net.ipv4.tcp_no_metrics_save = 1

# Turn on window scaling which can be an option to enlarge the transfer window
net.ipv4.tcp_window_scaling = 1
